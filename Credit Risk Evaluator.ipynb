{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(Path('Resources/2019loans.csv'))\n",
    "test_df = pd.read_csv(Path('Resources/2020Q1loans.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean up column labels a little bit\n",
    "train_df = train_df.drop('Unnamed: 0', axis=1).set_index('index')\n",
    "test_df = test_df.drop('Unnamed: 0', axis=1).set_index('index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert categorical data to numeric and separate target feature for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the label to create the X data\n",
    "X_train = train_df.drop('loan_status', axis=1)\n",
    "# One-hot encoding the entire dataframe\n",
    "X_train_dummies = pd.get_dummies(X_train)\n",
    "# Converting output labels to 0 and 1\n",
    "y_train = LabelEncoder().fit_transform(train_df['loan_status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert categorical data to numeric and separate target feature for testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the label to create the X data\n",
    "X_test = test_df.drop('loan_status', axis=1)\n",
    "# One-hot encoding the entire dataframe\n",
    "X_test_dummies = pd.get_dummies(X_test)\n",
    "# Converting output labels to 0 and 1\n",
    "y_test = LabelEncoder().fit_transform(test_df['loan_status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add missing dummy variables to testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# borrowed from https://stackoverflow.com/questions/41335718/keep-same-dummy-variable-in-training-and-testing-data\n",
    "# Get missing columns in the training test\n",
    "missing_cols = set( X_train_dummies.columns ) - set( X_test_dummies.columns )\n",
    "# Add a missing column in test set with default value equal to 0\n",
    "for c in missing_cols:\n",
    "    X_test_dummies[c] = 0\n",
    "# Ensure the order of column in the test set is in the same order than in train set\n",
    "X_test_dummies = X_test_dummies[X_train_dummies.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction \n",
    "I believe that, since we have a high number of explanatory variables in our dataset, that the random forest model should perform better than the logistic regression. However, I believe that if we had fewer dimensions to our dataset, it's possible that the logistic regression would perform similarly to the random forest classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Logistic Regression model on the unscaled data and print the model score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sceli\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train_dummies, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Score: 0.6507389162561577\n",
      "Testing Data Score: 0.5168013611229264\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training Data Score: {classifier.score(X_train_dummies, y_train)}\")\n",
    "print(f\"Testing Data Score: {classifier.score(X_test_dummies, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Random Forest Classifier model and print the model score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 1.0\n",
      "Testing Score: 0.6433432581880051\n"
     ]
    }
   ],
   "source": [
    "clf_1 = RandomForestClassifier(random_state=1, n_estimators=500).fit(X_train_dummies, y_train)\n",
    "print(f'Training Score: {clf_1.score(X_train_dummies, y_train)}')\n",
    "print(f'Testing Score: {clf_1.score(X_test_dummies, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "Although neither model had a particularly high (or, rather, satisfactory) accuracy score, I was correct in suggesting the random forest model would perform better. Whether that is due to my explanation, though, is unknown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train_dummies)\n",
    "X_train_scaled = scaler.transform(X_train_dummies)\n",
    "X_test_scaled = scaler.transform(X_test_dummies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Logistic Regression model on the scaled data and print the model score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sceli\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "Again, I think that the random forest model will perform better than the logistic regression. However, scaling the data may help our logistic regression perform better than the previous model. In either case, I feel that scaling the data will benefit both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Score: 0.7078817733990148\n",
      "Testing Data Score: 0.767333049766057\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training Data Score: {classifier.score(X_train_scaled, y_train)}\")\n",
    "print(f\"Testing Data Score: {classifier.score(X_test_scaled, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Random Forest Classifier model on the scaled data and print the model score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 1.0\n",
      "Testing Score: 0.6420672054444917\n"
     ]
    }
   ],
   "source": [
    "clf_2 = RandomForestClassifier(random_state=1, n_estimators=500).fit(X_train_scaled, y_train)\n",
    "print(f'Training Score: {clf_2.score(X_train_scaled, y_train)}')\n",
    "print(f'Testing Score: {clf_2.score(X_test_scaled, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "It appears that my prediction was wrong. The logistic regression actually outperformed the random forest model. Scaling the data didn't seem to have any affect on the accuracy of the random forest model's ability to perform predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.62679463e-02 3.45485047e-02 2.98657372e-02 1.46144695e-02\n",
      " 1.61086591e-02 2.93790795e-03 4.24775904e-03 8.90605979e-03\n",
      " 1.45871591e-03 1.52099714e-02 1.18468431e-02 2.94155218e-02\n",
      " 2.94587834e-02 4.35297967e-02 4.38158387e-02 5.11266480e-02\n",
      " 4.85751463e-02 1.63380968e-02 0.00000000e+00 0.00000000e+00\n",
      " 9.50429837e-02 7.68347729e-04 0.00000000e+00 4.85641668e-06\n",
      " 4.48273778e-03 1.36502159e-02 4.70734340e-03 6.36931003e-03\n",
      " 4.11118884e-03 6.44932197e-03 1.19289920e-02 1.35889955e-02\n",
      " 1.40696769e-02 4.83647186e-03 7.45463259e-03 1.69871914e-02\n",
      " 1.33222729e-02 1.54842126e-02 6.50752036e-03 6.72380732e-03\n",
      " 7.94603986e-03 9.50606273e-03 1.41975228e-02 1.57105454e-02\n",
      " 1.44976314e-02 2.61747130e-04 1.10129617e-06 1.58065408e-02\n",
      " 1.70075638e-02 1.19802666e-02 1.00446397e-02 5.62887191e-03\n",
      " 1.33503761e-02 1.21188589e-02 3.46059741e-03 6.97577709e-03\n",
      " 7.79427623e-03 7.60433284e-03 9.23415471e-03 1.09188155e-02\n",
      " 8.62871111e-03 1.02816420e-02 7.63861169e-03 8.86113480e-03\n",
      " 0.00000000e+00 1.05099196e-05 9.68549416e-04 6.69191747e-03\n",
      " 8.88265624e-03 8.23456327e-03 1.40248337e-03 0.00000000e+00\n",
      " 1.39631737e-02 1.38367574e-02 1.58523928e-02 1.38880185e-02\n",
      " 2.22175678e-04 1.90045758e-03 1.60884470e-03 1.73608122e-03\n",
      " 1.99224304e-03 1.98400615e-03 1.72042576e-03 0.00000000e+00\n",
      " 1.20325813e-03 1.19121274e-03 1.50850807e-03 1.55443237e-03\n",
      " 2.67261862e-03 2.68227990e-03 3.34084619e-05 4.07186561e-05]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAO/klEQVR4nO3db4hdd53H8ffHxKBtkbrb2aUm2Z0IoRoEtyW00S6yWBeSVsw+cNkWat2ChEK7VnFxo09kHyz0gYgtlITQViwWy1ILG2ywLlVhfdDStJVqjGGH2G3Gxu3IYhX7IAa/++Ce2tvpZO5J5s+d+d33C4bcc36/M/d7fpn7uWd+95wzqSokSe16y7gLkCStLINekhpn0EtS4wx6SWqcQS9Jjds47gIWctlll9X09PS4y5CkdeOZZ575VVVNLdS2JoN+enqao0ePjrsMSVo3kvzPudqcupGkxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMatyStjtXTT+x/74+MX7rphjJVIGjeP6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktS4XkGfZHeSE0lmkuxfoD1J7unan09y1VDbZ5McS/KTJN9M8rbl3AFJ0uJGBn2SDcC9wB5gB3BTkh3zuu0Btndf+4AD3babgU8DO6vqfcAG4MZlq16SNFKfI/qrgZmqOllVZ4CHgb3z+uwFHqyBJ4FLk1zetW0E3p5kI3AR8NIy1S5J6qFP0G8GTg0tz3brRvapql8AXwZeBE4Dr1TVdxd6kiT7khxNcnRubq5v/ZKkEfoEfRZYV336JHkng6P9bcC7gIuT3LzQk1TVoaraWVU7p6amepQlSeqjT9DPAluHlrfw5umXc/X5CPDzqpqrqt8DjwIfvPByJUnnq0/QPw1sT7ItySYGH6YentfnMHBLd/bNLgZTNKcZTNnsSnJRkgDXAceXsX5J0ggbR3WoqrNJ7gAeZ3DWzANVdSzJbV37QeAIcD0wA7wK3Nq1PZXkEeBZ4CzwHHBoJXZEkrSwkUEPUFVHGIT58LqDQ48LuP0c234J+NISapQkLYFXxkpS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY3rFfRJdic5kWQmyf4F2pPknq79+SRXDbVdmuSRJD9LcjzJB5ZzByRJixsZ9Ek2APcCe4AdwE1JdszrtgfY3n3tAw4Mtd0NfKeq3gO8Hzi+DHVLknrqc0R/NTBTVSer6gzwMLB3Xp+9wIM18CRwaZLLk7wD+BBwP0BVnamqXy9f+ZKkUfoE/Wbg1NDybLeuT593A3PA15I8l+S+JBcv9CRJ9iU5muTo3Nxc7x2QJC2uT9BngXXVs89G4CrgQFVdCfwOeNMcP0BVHaqqnVW1c2pqqkdZkqQ++gT9LLB1aHkL8FLPPrPAbFU91a1/hEHwS5JWSZ+gfxrYnmRbkk3AjcDheX0OA7d0Z9/sAl6pqtNV9UvgVJIrun7XAT9druIlSaNtHNWhqs4muQN4HNgAPFBVx5Lc1rUfBI4A1wMzwKvArUPf4p+Ah7o3iZPz2iRJK2xk0ANU1REGYT687uDQ4wJuP8e2PwJ2XniJkqSl8MpYSWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0a9T0/seY3v/YuMuQ1ACDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQb9OuAZOJKWwqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGbRx3ARoYvvL1hbtuGGMlklpj0I+RtzWQtBqcupGkxhn0ktQ4g16SGtcr6JPsTnIiyUyS/Qu0J8k9XfvzSa6a174hyXNJvr1chUuS+hkZ9Ek2APcCe4AdwE1JdszrtgfY3n3tAw7Ma78TOL7kaiVJ563PEf3VwExVnayqM8DDwN55ffYCD9bAk8ClSS4HSLIFuAG4bxnrliT11CfoNwOnhpZnu3V9+3wV+DzwhwsrUZK0FH2CPgusqz59knwUeLmqnhn5JMm+JEeTHJ2bm+tRliSpjz5BPwtsHVreArzUs8+1wMeSvMBgyufDSb6x0JNU1aGq2llVO6empnqWL0kapU/QPw1sT7ItySbgRuDwvD6HgVu6s292Aa9U1emq+kJVbamq6W6771XVzcu5A5KkxY28BUJVnU1yB/A4sAF4oKqOJbmtaz8IHAGuB2aAV4FbV65kSdL56HWvm6o6wiDMh9cdHHpcwO0jvscPgB+cd4WSpCXxylhJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhrX9N+M9Q9uS5JH9JLUPINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1Limr4wd5lWykibVxAT9uMx/g3lt2TcbSavFqRtJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuN6BX2S3UlOJJlJsn+B9iS5p2t/PslV3fqtSb6f5HiSY0nuXO4dkCQtbmTQJ9kA3AvsAXYANyXZMa/bHmB797UPONCtPwt8rqreC+wCbl9gW0nSCupzRH81MFNVJ6vqDPAwsHden73AgzXwJHBpksur6nRVPQtQVb8FjgObl7F+SdIIfYJ+M3BqaHmWN4f1yD5JpoErgacWepIk+5IcTXJ0bm6uR1mSpD76BH0WWFfn0yfJJcC3gM9U1W8WepKqOlRVO6tq59TUVI+yJEl99An6WWDr0PIW4KW+fZK8lUHIP1RVj154qZKkC9En6J8GtifZlmQTcCNweF6fw8At3dk3u4BXqup0kgD3A8er6ivLWrkkqZeNozpU1dkkdwCPAxuAB6rqWJLbuvaDwBHgemAGeBW4tdv8WuATwI+T/Khb98WqOrKseyFJOqeRQQ/QBfOReesODj0u4PYFtvshC8/fS5JWiVfGSlLjeh3RS+Myvf8xAF6464YxVzLaa7XC+qhXk6O5oF9PwXAhDBNJ56u5oL8QhqcuVOsHFmuBr8+lM+i1Li33i98wUcv8MHaZTO9/7A1hIUlrxcQGvcGsxfjzoZY4daOxWAtTJQb52uLnHSvHoNc5LfcLzxey+jrXz8paOEBYjwx6rRstHoEbXFoNEztHL0mTwqCX1hA/BNZKcOpmBfhCHZjkaYlJ3netPR7RS1LjPKKXGuVvFXqNQS9NIN8EJotTN9Iq8ENWjZNBr4li4GoSOXVzHvx1V2vdcryJLfUKZl8na49H9NKE8LeZyeURvZrm0eX4eY+j8TPopXVm/puXUy0axakbrTlOMUjLyyN6NafFN4m1vk9rvb5JZ9DrvC32q/64XvDOA0vn5tSNJDXOI3pplTnNodVm0GvsPOtDWlkGvf5oLc69S1o65+hH8FS/0RwjaW0z6KU1yjdQLRenbuZxvlhSawx6SVohw9d3jPMg0qCXNFHONR3W8m/wBr2kJiwW4H2vnG71BnG9gj7JbuBuYANwX1XdNa89Xfv1wKvAP1bVs322Xa/W6n+otJat19fNYncMXQ8fmI8M+iQbgHuBvwVmgaeTHK6qnw512wNs776uAQ4A1/TcVitsvb64tLZ5f6H1o88R/dXATFWdBEjyMLAXGA7rvcCDVVXAk0kuTXI5MN1jW2lN841y7fLNpp8MsnmRDsnHgd1V9alu+RPANVV1x1CfbwN3VdUPu+UngH9hEPSLbjv0PfYB+7rFK4ATS9ivy4BfLWH7VjgOr3MsBhyHgRbH4S+ramqhhj5H9Flg3fx3h3P16bPtYGXVIeBQj3pGSnK0qnYux/dazxyH1zkWA47DwKSNQ5+gnwW2Di1vAV7q2WdTj20lSSuozy0Qnga2J9mWZBNwI3B4Xp/DwC0Z2AW8UlWne24rSVpBI4/oq+pskjuAxxmcIvlAVR1LclvXfhA4wuDUyhkGp1feuti2K7Inb7QsU0ANcBxe51gMOA4DEzUOIz+MlSStb969UpIaZ9BLUuOaC/oku5OcSDKTZP+461ktSbYm+X6S40mOJbmzW/8nSf4zyX93/75z3LWuhiQbkjzXXeMxkePQXbj4SJKfdT8XH5jEcQBI8tnudfGTJN9M8rZJGoumgn7olgt7gB3ATUl2jLeqVXMW+FxVvRfYBdze7ft+4Imq2g480S1PgjuB40PLkzgOdwPfqar3AO9nMB4TNw5JNgOfBnZW1fsYnBhyIxM0Fk0FPUO3a6iqM8Brt1xoXlWdfu1GclX1WwYv6s0M9v/rXbevA383lgJXUZItwA3AfUOrJ2ockrwD+BBwP0BVnamqXzNh4zBkI/D2JBuBixhczzMxY9Fa0G8GTg0tz3brJkqSaeBK4Cngz7trGuj+/bMxlrZavgp8HvjD0LpJG4d3A3PA17oprPuSXMzkjQNV9Qvgy8CLwGkG1/l8lwkai9aCvvctF1qV5BLgW8Bnquo3465ntSX5KPByVT0z7lrGbCNwFXCgqq4EfkfDUxOL6ebe9wLbgHcBFye5ebxVra7Wgr7P7RqaleStDEL+oap6tFv9v92dROn+fXlc9a2Sa4GPJXmBwdTdh5N8g8kbh1lgtqqe6pYfYRD8kzYOAB8Bfl5Vc1X1e+BR4INM0Fi0FvQTe8uF7o+/3A8cr6qvDDUdBj7ZPf4k8B+rXdtqqqovVNWWqppm8P//vaq6mckbh18Cp5Jc0a26jsHtwSdqHDovAruSXNS9Tq5j8BnWxIxFc1fGJrmewRzta7dc+LfxVrQ6kvw18F/Aj3l9bvqLDObp/x34CwY/8H9fVf83liJXWZK/Af65qj6a5E+ZsHFI8lcMPpDeBJxkcGuStzBh4wCQ5F+Bf2BwdtpzwKeAS5iQsWgu6CVJb9Ta1I0kaR6DXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXu/wF9Nbf7emdlkwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "features = clf_2.feature_importances_\n",
    "print(features)\n",
    "plt.bar(x = range(len(features)), height=features)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
